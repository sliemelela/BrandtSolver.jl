# Theory
To understand how the package works, we will treat the algorithm proposed by
[Brandt_etal_2005](@cite) step by step.


## Goal of the algorithm
In this package we are treating a terminal wealth optimization problem.
More specifically, in this package we consider portfolio choice problems at timesteps
$n = 1, 2, \ldots, M$, where $M + 1$ is some terminal timestep.
This portfolio choice problem at timestep $n$ is defined by an investor who maximizes the expected utility
of their wealth at the terminal date $M$ by trading $N$ risky assets and a risk-free asset (cash).
Formally the investor's problem at timestep $n$ is
```math
    V_n(W_n, Z_n)
    = \max_{\{\omega_s\}_{s = n}^{M- 1}} \mathbb{E}_n[u(W_M)]
```
subject to the sequence of budget constraints
```math
    W_{m + 1} = W_m (\omega_s^\top R^e_{m + 1} + R_{m + 1})
```
for all $s \geq n$.
Here $R^e_{m + 1}$ can be interpreted as the excess return of the risky assets over the risk-ree
asset, and $R_{m + 1}$ is the gross return of other processes that _may_ depend on wealth
$W_m$.
Furthermore, $\{\omega_s\}_{s=n}^{M}$ is the sequence of portfolio weights chosen at times
$m = n, \ldots, M$ and $u$ is the investor's utility function.
The process $Z_n$ is a vector of state variables that are relevant for the investor's decision making.
Lastly, the function $u$ denotes the utility function of the investor.
The goal of this package is to find $\{\omega_s\}_{s=1}^{M}$

## Solution of the algorithm
The goal of this section is to give the final solution of the algorithm.
We acknowledge that the result can look quite daunting and so it is not the expectation that the
reader can immediately make sense of why the solution looks like it does.
Despite this, it can help to first see where we are working towards before explaining how it is
derived. Let us first start by specifying the notation.


### Notation
Let $\omega_m = (\omega^1_m, \ldots, \omega^N_m)$ be the components of the portfolio weights
at some timestep $m$ and write $R^e_{m + 1} = (R^{e, 1}_{m + 1}, \ldots, R^{e, N}_{m + 1})$
for the components for the excess return vector corresponding to each asset that is traded.
If $\omega_m$ is chosen optimally, we decorate it with a $\star$ in the superscript, i.e.
$\omega_m^{\star}$ is the vector of optimal portfolio weights at timestep $m$.
Furthermore, let us assume that the current timestep is denoted by $n$ and let us also define
```math
    \hat W_{M + 1} = W_n R_{n + 1}
        \prod_{m = n + 1}^{M} ((\omega_m^{\star})^\top R^e_{m + 1} + R_{s + 1}).
```

### Result
We assume that the current timestep is denoted by $n$ and that all optimal portfolio weights at
times $m > n$ are known, i.e. $\{\omega_m^\star\}_{m = n + 1}^{M}$ are known.
Assuming the value function $V$ is infinitely differentiable in the first argument, the optimal
portfolio weights $\omega_n^\star$ is the limit of the solutions of the equation indexed by
$k$
```math
\begin{aligned}
    \sum_{r = 1}^{k} \frac{W_t^{r - 1}}{(r - 1)!}  &\Biggl(\sum_{k_1 + \ldots + k_N = r - 1}
    \binom{r - 1}{k_1, \ldots, k_N} \prod_{i=1}^N (\omega_n^i)^{k_i} \times \\
   &\mathbb{E}_n \left[\partial^{r} u(\hat W_{M + 1})
      \prod_{m = n + 1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1})^r
      \prod_{i=1}^N (R^i_{n + 1})^{k_i} R^e_{n+1}\right]\Biggr) = 0,
\end{aligned}
```
where $\binom{p}{k_1,\ldots, k_N}$ is the multinomial given by
```math
  \binom{p}{k_1,\ldots, k_N} = \frac{p!}{k_1 ! k_2 ! \cdots k_N !}.
```

Given this result, the pragmatic reader may come up with some reasonable questions:
- How should one choose $k$?
- How should one calculate the conditional expectation value $\mathbb{E}_n[\cdot]$?
- How should one find the optimal portfolio weights $\omega_m^\star$ for $m > n$?

The short and concise answers are:
- The higher $k$, the better. In our experience $4$ suffices for most applications.
- This is done by means of regressions. This can be unstable however.
    Dropping part of the data generated by the integrand $\cdot$ in $\mathbb{E}_n[\cdot]$ seems to partly
    solve this issue.
- This is done by first solving for $\omega_M^\star$. You can then recursively find the solution for
    any time.

If the above does not make sense, do not fret.
This will soon all make sense once we treat the derivation of the above.

## Deriving the solution: so, how do we get there?
The (rough) outline of the derivation is as follows:
1. We prove the value function satisfies the Bellman equation
```math
V_n(W_n, Z_n) = \max_{\omega_n} \mathbb{E}_n[V_{n + 1}(W_{n + 1}, Z_{n + 1})]
```
2. We derive the Taylor expansion the value function $V_{n + 1}$ in the conditional expectation.
    This gives a new expression for $V_n$.
    _Note_: This is where the $k$ came from in the [solution](#result).
3. Using the new expression for $V_n$ we derive the first order conditions of finding the optimal
    $\omega_n$.
4. We manipulate the first order conditions so that all unknown quantities are replaced by
    known quantities.

### The Bellman equation
The value function satisfies the Bellman equation
```math
  V_n(W_n, Z_n) = \max_{\omega_n} \mathbb{E}_n[V_{n + 1}(W_{n + 1}, Z_{n + 1})]
```
with terminal condition $V_{M + 1} (W_{M + 1}, Z_{M + 1}) = u(W_{M + 1})$.

#### Proof
We see that
```math
\begin{aligned}
    V_n(W_n, Z_n)
    &= \max_{\{\omega_m\}_{m = n}^{M}} \mathbb{E}_n\left[ u(W_{M + 1}) \right]\\
    &= \max_{\omega_n} \max_{\{\omega_m\}_{m = n + 1}^{M}} \mathbb{E}_n\left[ u(W_{M + 1}) \right] \\
    &= \max_{\omega_n} \max_{\{\omega_m\}_{m = n + 1}^{M}}
        \mathbb{E}_n\left[  \mathbb{E}_{n + 1}\left[ u(W_{M + 1}) \right] \right] \\
    &= \max_{\omega_n} \mathbb{E}_n\left[ \max_{\{\omega_m\}_{m = n + 1}^{M}}
        \mathbb{E}_{n + 1}\left[ u(W_{M + 1}) \right] \right] \\
    &= \max_{\omega_n} \mathbb{E}_n\left[ V_{n + 1}(W_{n + 1}, Z_{n + 1}) \right].
\end{aligned}
```
The first equality follows from the definition of the value function.
The second equality follows from separating the maximization over $\omega_t$ and the remaining
maximization over $\{\omega_m\}_{m = n + 1}^{M}$.
The third equality follows from the law of iterated expectations.
The fourth equality follows from the fact that the maximization over
$\{\omega_m\}_{m = n + 1}^{M}$ is independent of the choice of $\omega_n$ and can thus be moved inside the expectation.
And the last equality follows from the definition of the value function at time $t + 1$.

### Taylor expanding the value function
Let us assume that the value function $V$ is $C^(k + 1)$ in the first argument for $k \geq 2$.
Then the value function (using the Bellman equation) satisfies
```math
\begin{aligned}
    V_n(W_n, Z_n)
    &= \max_{\omega_n} \Biggl\{ \sum_{r = 0}^{k} \frac{W_n^r}{r!} \mathbb{E}_n \left[ \partial_1^r V_{n+1}(W_n R_{n+1}, Z_{n+1}) (\omega_n^\top R^e_{n+1})^r \right] \\
    &+ \frac{W_n^{k+1}}{(k+1)!} \mathbb{E}_n \left[ \partial_1^{k+1} V_{n+1}(\xi, Z_{n+1}) (\omega_n^\top R^e_{n+1})^{k+1} \right]
    \Biggr\}
\end{aligned}
```
for some $\xi$ between $W_n R_{n + 1}$ and $W_n (\omega_n^\top R^e_{n + 1} + R_{n + 1})$.
Here we assume that all moments exist and are finite.

#### Proof
Let us consider the mapping $f(x) = V_{n + 1} (x, Z_{n + 1})$ for fixed $n$ and $Z_{n + 1}$.
Using Taylor's theorem, we have that
```math
    f(x) = \sum_{r = 0}^k \frac{1}{r!} f^{(r)}(x_0)(x - x_0)^r + \frac{f^{(k + 1)}(\xi)}{(k+1)!}(x - x_0)^{k + 1}
```
for some $\xi$ between $x$ and $x_0$.
Taking $x_0 = W_n R_{n + 1}$ and $x = W_{n + 1}$, and noting that
$W_{n + 1} - W_n R_{n + 1} = W_n \omega_n^\top R^e_{n + 1}$ by the budget constraint,
the result follows.

### Finding the first order conditions
The next is to solve the static maximization problem in the Taylor expansion.
The next proposition provides the final equation for this.

Let $\omega_n = (\omega_n^1, \dots, \omega_n^N)$ be the components of the portfolio weights at time $n$.
Similarly, write $R^e_{n+1} = (R_{n+1}^{e,1}, \dots, R_{n+1}^{e,N})$.
The first order conditions (FOC) are given by:
```math
   \sum_{r = 1}^{k} \frac{W_n^{r - 1}}{(r - 1)!} \mathbb{E}_n \left[ \partial_1^r V_{n + 1}(W_n R_{n + 1}, Z_{n + 1}) (\omega_n^\top R^e_{n + 1})^{r - 1} R^e_{n + 1} \right] + \frac{W_n^k}{k!} \mathbb{E}_n \left[ \partial_1^{k + 1} V_{n + 1}(\xi, Z_{n + 1}) (\omega_n^\top R^e_{n + 1})^{k} R^e_{n + 1} \right] = 0
```
or alternatively:
```math
\begin{aligned}
   \sum_{r = 1}^{k} \frac{W_n^{r - 1}}{(r - 1)!}  &\Biggl(\sum_{k_1 + \dots + k_N = r - 1} \binom{r - 1}{k_1, \dots, k_N} \prod_{i=1}^N (\omega_n^i)^{k_i} \times \\
   &\mathbb{E}_n \left[ \partial_1^r V_{n + 1}(W_n R_{n + 1}, Z_{n + 1}) \prod_{i=1}^N (R^{e, i}_{n + 1})^{k_i} R^e_{n+1} \right]\Biggr) \\
    &+ \frac{W_n^k}{k!} \mathbb{E}_n \left[ \partial_1^{k + 1} V_{n + 1}(\xi, Z_{n + 1}) (\omega_n^\top R^e_{n + 1})^{k} R^e_{n + 1} \right] = 0.
\end{aligned}
```

#### Proof
To derive the FOC, we take the static maximization problem from the Taylor expansion,
take the derivative with respect to $\omega_n$, and set it to $0$.
After this, we divide both sides by $W_n$. This yields the first result.

We now note that for arbitrary $r \in \mathbb{N}$, it holds by the multinomial theorem that:
```math
(\omega_n^\top R^e_{n + 1})^{r-1} = \sum_{k_1 + \dots + k_N = r-1}
    \binom{r-1}{k_1, \dots, k_N} \prod_{i=1}^N (\omega_n^i R^{e, i}_{n + 1})^{k_i}
```
where $\binom{p}{k_1, \dots, k_N}$ is the multinomial coefficient.
Using this identity and the linearity of the expectation, we find the second result.

### Finding an alternative way of writing the value function
The last problem we ought to solve is the problem of having no expression for the derivatives of the
value function $\partial^r_1 V_{n + 1}(W_n R_{n+1}, Z_{n + 1})$. To that end, we note the following lemma.

Let $\{\omega^\star_m\}_{m = n + 1}^{M}$ be the optimal sequence of portfolio weights at times $m = n + 1, \dots, M$,
and denote:
```math
\hat{W}_{M+1} = W_n R_{n + 1} \prod_{m = n+1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1}).
```
Then:
```math
\partial^{r}_1 V_{n + 1}(W_n R_{n + 1}, Z_{n + 1}) = \mathbb{E}_{n + 1} \left[ u^{(r)}(\hat{W}_{M+1}) \prod_{m = n + 1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1})^r \right].
```


#### Proof
Using the definition of the value function and assuming $\{\omega^\star_m\}$ is the optimal sequence, we have:
```math
    V_{n + 1}(x, Z_{n + 1}) = \mathbb{E}_{n + 1} \left[ u \left( x \prod_{m = n+1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1}) \right) \right].
```
Differentiating $r$ times with respect to $x$ inside the expectation yields the result.
Taking $x = W_n R_{n + 1}$ completes the proof.



### Final result
Using this, we can now reformulate the FOC.

The first order conditions (FOC) are given by:
```math
\begin{aligned}
   \sum_{r = 1}^{k} \frac{W_n^{r - 1}}{(r - 1)!}  &\Biggl(\sum_{k_1 + \dots + k_N = r - 1} \binom{r - 1}{k_1, \dots, k_N} \prod_{i=1}^N (\omega_n^i)^{k_i} \times \\
   &\mathbb{E}_n \left[ u^{(r)}(\hat{W}_{M+1}) \prod_{m = n + 1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1})^r \prod_{i=1}^N (R^{e, i}_{n + 1})^{k_i} R^e_{n+1} \right]\Biggr) \\
    &+ \text{Remainder} = 0.
\end{aligned}
```

### Implementation concerns
In most polynomial solvers, it is necessary to provide an initial guess.
To that end, we consider the $2$-nd order expansion ($k=2$), which yields the linear FOC:
```math
  a_n + B_n \omega_n = 0 \implies \omega_n = -B_n^{-1} a_n
```
where $a_n$ is a vector and $B_n$ is a matrix with columns $b_{i, n}$ given by
```math
\begin{aligned}
  a_{n} &= \mathbb{E}_n \left[ u'(\hat{W}_{M+1}) \prod_{m = n + 1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1}) R^e_{n+1} \right], \\
  b_{i, n} &= \mathbb{E}_n \left[ u''(\hat{W}_{M+1}) \prod_{m = n + 1}^{M} ((\omega_m^\star)^\top R^e_{m + 1} + R_{m + 1})^2 R^{e, i}_{n + 1} R^e_{n+1} \right].
\end{aligned}
```







## References
```@bibliography
```